{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Feladat megfogalmazása:\n",
    "#\n",
    "# Szöveges mintákhoz tartozó értékek beazonosítása. A minták megtalálása a sok variáció miatt klasszikus módszerrel nehéz.\n",
    "\n",
    "# 1.1 Lehetséges minták például:\n",
    "#\n",
    "# \"az arteria carotis communisban ... % stenosist okozó echogen plaque.\"\n",
    "# \"az arteria carotis interna elején ... % stenosist okozó echogen plaque.\",\n",
    "# \"carotis oszlásban (bifurcatioban) ... % stenosist okozó echogen plaque.\",\n",
    "# \"a carotis externában ... % stenosist okozó echogen plaque.\",\n",
    "# \"arteria vertebralisban az áramlási sebesség a norm. érték alsó határának ... %-a.\",\n",
    "# \"arteria basilarisban az áramlási sebesség a norm. érték alsó határának ... %-a.\n",
    "\n",
    "# 1.2 A ... -helyeken bármilyen érték álhat:\n",
    "#\n",
    "# Példák a lehetséges értékekre:\n",
    "# 10, 20, 10-20, kb.10, kb.50 stb...\n",
    "\n",
    "# 1.3 A minták időben lehet módosulta. A módosulás mértékről nincs információ:\n",
    "#\n",
    "# Példa a modósulásara: Az \"echogen\" szó felcserélődött a \"meszes\" szóra\n",
    "# Például következő minták szemantikai jelentése azonos.\n",
    "# \"az arteria carotis communisban ... % stenosist okozó echogen plaque.\"\n",
    "# \"az arteria carotis communisban ... % stenosist okozó meszes plaque.\"\n",
    "# \"az arteria carotis communisban hosszában ... % stenosist okozó meszes plaque.\"\n",
    "\n",
    "# 1.4 Teljes példa egy feldolgozandó szöveg részre:\n",
    "#\n",
    "# Endarterectomia utáni állapot. Az arteria carotis communis 10-20 % stenosist okozó meszes plaque. A carotis oszlásban (bifurcatioban) 20-30 % stenosist okozó meszes plaque. Az arteria carotis interna elején 20 % stenosist okozó meszes plaque.\n",
    "#\n",
    "# Felismerendő minta: Az arteria carotis communis 10-20 % stenosist okozó meszes plaque.\n",
    "# Kiolvasandó érték: 10-20\n",
    "\n",
    "# 1.5 Architektúra:\n",
    "#\n",
    "# A feladatot megoldó neurális hálózat általánosítható több minta felépítésére.\n",
    "#\n",
    "# Klasszikus értelemben vett címkézett adatok nem állnak rendelkezésre. A hálózatnak magának kell megtanulnia mely szekvencia minta és mely nem az.\n",
    "# \n",
    "# A címkézés hiányátt a következő trükkel oldhatjuk fel:\n",
    "#\n",
    "# Két minta akkor van közel egymáshoz ha bennük lévő szavak jelentése és sorrendisége megegyezik. A szemantikai egyezés úgy vizsgáltam, hogy a cél mintákban és tanuló adathalmazban lévő szavakat beágyazással vektorizáltam. Ezután két mondat szemantikai egyezőségét az fogja megadni, hogy a cosinus hasonlósága átlagosan milyen messze van a mondatokban azonos helyen lévő szavaknak.\n",
    "#\n",
    "# Például legyen a minták és a vizsgálandó szövegrész a következő:\n",
    "# Minta: arteria carotis communis\n",
    "# Szöveg: arteria carotis hosszában\n",
    "# A minta és a szöveg első két szava megegyezik tehát ott a coszinus hasonlóság 1 még az utolsó szavak esetében valamilyen -1-nél nagyobb de 1 nél kisebb szám lesz. Ha ezeket összeadjuk és elosszuk a hosszal ami jelen példában 3 megkapjuk az átlagos hasonlóságát a két mondatnak. \n",
    "# A cosinus hasonlóság mindig -1 és 1 közötti szám ami nem előnyös ezért azt a következő képlettel 0 és 1 köz szorítom: (1 + cos_sim / 2)\n",
    "#\n",
    "# Ha ezzel a módszerrel előállítjuk a címkéket akkor a tanuló halmaz összes szekvenciájához (amit úgy állítunk elő hogy ugyanolyan hosszú darabokra tördeljük a forrás szöveget mint amilyen a minta hossza) rendelünk egy 0 és 1 közötti számot. Ez a szám megadja majd, hogy az adott szekvencia milyen közel van a mintánkhoz. \n",
    "#\n",
    "# Az előállított adathalmazhoz egyszerűen építhető egy komplex háló ami akár 99.99% pontosággal rá tud tanulni a címkékre. A háló felépítésénél az fontos, hogy mindenképpen tartalmaznia kell egy Embedding layer-t a beágyazás miatt és egy LSTM háló a szavak szórendiségének figyelése miatt. Egy ’sima’ sűrűn kapcsolt hálónál ez a felépítés lényegesen jobban működött.\n",
    "#\n",
    "# Az aktivációs függvény sigmoid emiatt kell 0 és 1 közé szorítani a cosinus hasonlóságot. Veszteség függvénynek mean squared error. Metrika pedig a Root Mean Squared Error vagy Mean Absolute Error is megfelelő. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                             36&&1.21\n1                                               90&&50\n2                                               34&&21\n3    Endarterectomia utáni állapot. Az arteria caro...\n4    Az arteria carotis communis egész hosszában 10...\n5                                   NORMÁLIS&&NORMÁLIS\n6                                               67&&39\n7                                               54&&27\n8    A subclaviaban signifikáns szűkületre utaló ár...\n9                                                _igen\nName: obsval, dtype: object"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# 2.0 Adatok előkszítése\n",
    "import pandas\n",
    "\n",
    "soruce = \"sources/source_idydptc_0_10000.xlsx\"\n",
    "data = pandas.DataFrame()\n",
    "xl = pandas.ExcelFile(soruce)\n",
    "parse = xl.parse(xl.sheet_names)\n",
    "sheet = xl.sheet_names[0]\n",
    "data = parse[sheet]\n",
    "# 2.1 Az adathalmaz első 10 sorra. Egy lehetséges minta a 3. és 4 sorokban található\n",
    "data[\"obsval\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "36 1.21\n90 50\n34 21\nendarterectomia utáni állapot. az arteria carotis communis egész hosszában 10-20 % stenosist okozó meszes plaque. a carotis oszlásban bifurcatioban 20-30 % stenosist okozó meszes p\n"
    }
   ],
   "source": [
    "# 3.0 corpus elkészítes word2vec-hez\n",
    "from keras_preprocessing import text\n",
    "\n",
    "output_corp = open(\"sources/corpus.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "lenv = len(data[\"obsval\"])\n",
    "my_filters = '\"#$&()*+/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "for i in range(0, lenv):\n",
    "    tmp = str(data[\"obsval\"][i]).lower()\n",
    "    tmp = text.text_to_word_sequence(text=tmp,\n",
    "                                        filters=my_filters)\n",
    "    tmp = \" \".join(map(str, tmp))\n",
    "    output_corp.write(tmp + \"\\n\")\n",
    "\n",
    "output_corp.close()\n",
    "\n",
    "# 3.1 A corpus első 200 karaktere\n",
    "f = open(\"sources/corpus.txt\", \"r\", encoding=\"utf-8\")\n",
    "print(f.read()[:200])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "carotis\tendarterectomia utáni állapot. az arteria communis egész hosszában 10-20\ncommunis\tutáni állapot. az arteria carotis egész hosszában 10-20 %\negész\tállapot. az arteria carotis communis hosszában 10-20 % stenosist\nhosszában\taz arteria carotis communis egész 10-20 % stenosist okozó\n10-20\tarteria carotis communis egész hosszában % stenosist okozó meszes\n%\tcarotis communis egész hosszában 10-20 stenosist okozó meszes plaque.\nstenosist\tcommunis egész hosszában 10-20 % okozó meszes plaque. a\noko\n"
    }
   ],
   "source": [
    "# 4.0 Minták betöltése és tanuló adathalmaz előkészítése:\n",
    "\n",
    "output_train = open(\"sources/corpus_train.txt\", \"w\", encoding=\"utf-8\")\n",
    "# \"next_tokens_count\" - A mintában hány szó van a kiolvasandó érték után\n",
    "next_tokent_count = 4\n",
    "# \"prev_tokens_count\" - A mintában hány szó van a kiolvasandó érték előtt\n",
    "prev_tokens_count = 5\n",
    "# Például az alábbi mintában a kiolvasandó érték \"az arteria carotis communisban\" szavak után van és a \"% stenosist okozó echogen plaque.\" szavak előtt.\n",
    "\n",
    "# sample_count - Megadja, hogy a tanuló adathalmazt mennyi plusz mintával dusítjuk.\n",
    "sample_count = 5000\n",
    "# \"sample\" - A minta összes szava\n",
    "sample = [\"az arteria carotis communisban % stenosist okozó echogen plaque.\"]\n",
    "for i in range(0, lenv):\n",
    "    try:\n",
    "        tmp_split = data[\"obsval\"][i].lower().split(\" \")\n",
    "        len_tmp = len(tmp_split)\n",
    "        for index in range(0, len_tmp):\n",
    "\n",
    "            tmp_next_tokent_count = index + 1 + next_tokent_count\n",
    "            tmp_prev_tokens_count = index - prev_tokens_count\n",
    "\n",
    "            if tmp_prev_tokens_count >= 0 and tmp_next_tokent_count < len_tmp and len(tmp_split[index]) > 0:\n",
    "                env = tmp_split[tmp_prev_tokens_count:index] + \\\n",
    "                    tmp_split[(index+1):tmp_next_tokent_count]\n",
    "\n",
    "                output_train.write(tmp_split[index] +\n",
    "                                \"\\t\" + \" \".join(env) + \"\\n\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for j in range(0, sample_count):\n",
    "    output_train.write(\"SAMPLE\\t\" + sample[0] + \"\\n\")\n",
    "\n",
    "output_train.close()\n",
    "\n",
    "# A tanuló adathalmaz első 500 karaktere\n",
    "f = open(\"sources/corpus_train.txt\", \"r\", encoding=\"utf-8\")\n",
    "print(f.read()[:500])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1759 16\n\na 1.2400651 1.3711394 1.261693 0.8345974 0.2602962 -5.0107775 2.0614645 0.43603092 1.0035423 1.3180683 -3.2980626 0.30995876 -0.03683576 -0.19721483 0.81641406 -0.018020403\n\n"
    }
   ],
   "source": [
    "# 5.0 Corpus beágyazása\n",
    "import gensim # https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/models/word2vec/index.html\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_file=\"sources/corpus.txt\",\n",
    "                                window=10,\n",
    "                                size=embedding_dim,\n",
    "                                iter=10,\n",
    "                                min_count=1)\n",
    "\n",
    "model.save(\"word2vec.model\")\n",
    "model.wv.save_word2vec_format(\"word2vec_pattern.vec\")\n",
    "\n",
    "# 5.1 A beágyazott szókészlet mérete és első szavának vektora\n",
    "f = open(\"word2vec_pattern.vec\", \"r\", encoding=\"utf-8\")\n",
    "print(f.readline())\n",
    "print(f.readline())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "endarterectomia utáni állapot. az arteria communis egész hosszában 10-20\n"
    }
   ],
   "source": [
    "# 6.0 Tanuló adatok betöltése és Tanuló/Teszt felbontás, tokenizálás elvégzése\n",
    "soruce = \"sources/corpus_train.txt\"\n",
    "samples = [\"az arteria carotis communisban % stenosist okozó echogen plaque.\"]\n",
    "maxlen = max(len(x.split(\" \")) for x in samples)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# 6.1 corpus beolvasása\n",
    "f = open(soruce, \"r\", encoding=\"utf-8\")\n",
    "tmp_buff = f.read().split(\"\\n\")\n",
    "lenb = len(tmp_buff)\n",
    "for i in range(0, lenb):\n",
    "    if(len(tmp_buff[i]) == 0):\n",
    "        continue\n",
    "\n",
    "    tmp_row = tmp_buff[i].split(\"\\t\")\n",
    "    x_train.append(tmp_row[1])\n",
    "f.close()\n",
    "\n",
    "# 6.2 A tanuló adathalmaz első eleme\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 91  79  77   2   7  35 140 151  25]\n [ 79  77   2   7   1 140 151  25   3]\n [ 77   2   7   1  35 151  25   3   4]\n [  2   7   1  35 140  25   3   4   6]]\n[[ 2  7  1 18  3  4  6 21  8]]\n"
    }
   ],
   "source": [
    "# 7.0 Tokenizálás\n",
    "\n",
    "import numpy\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, filters=my_filters)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "tmp_samples = tokenizer.texts_to_sequences(samples)\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "tmp_samples = pad_sequences(tmp_samples, padding='post', maxlen=maxlen)\n",
    "y_train = numpy.copy(x_train).astype(numpy.int32)\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "tokenizer_json = {\n",
    "    \"max_lenght\": maxlen,\n",
    "    \"vocab_size\": len(tokenizer.word_index) + 1,\n",
    "    \"tokens\": tokenizer_json\n",
    "}\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer,\n",
    "                handle,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 7.1 Az adathalmaz első 5 eleme tokenizálás után\n",
    "print(x_train[:4])\n",
    "# 7.2 A minta tokenizálás után\n",
    "print(tmp_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "56281\n14071\n"
    }
   ],
   "source": [
    "# 8.0 Tanuló adatok felbontása teszt és tanló adathalmazra 80/20 arányban\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    train_size=0.8,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1000)\n",
    "\n",
    "# Tanuló adathalmaz mérete\n",
    "print(len(x_train))\n",
    "# Teszt adathalmaz mérete\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 4.18737221  1.95336926  4.75430059 -0.88585562 -3.26712275 -1.85779703\n  0.34983012 -0.91726637 -0.45905134  3.02688694 -2.6880765   0.00664545\n  2.87261868 -1.41956437  2.16651821  0.96636367]\n"
    }
   ],
   "source": [
    "# 9.0 Az előre elkészített beágyazás betöltése\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = numpy.zeros((vocab_size, embedding_dim))\n",
    "embedding_matrix[0].fill(-1.0)\n",
    "\n",
    "f = open(\"word2vec_pattern.vec\", encoding='utf8', errors='ignore')\n",
    "index = 0\n",
    "for row in f:\n",
    "    word, *vector = row.split()\n",
    "    if word in tokenizer.word_index:\n",
    "        idx = tokenizer.word_index[word]\n",
    "        embedding_matrix[idx] = numpy.array(vector, dtype=numpy.float32)[:embedding_dim]\n",
    "\n",
    "    index = index + 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "# 9.1 Beágyazást táróló mátrix 2. eleme\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 9, 16)             23728     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 16)                2112      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 25,857\nTrainable params: 25,857\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# 10. Model összeállítása\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(len(tokenizer.index_word)+1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True))\n",
    "model.add(LSTM(embedding_dim))\n",
    "\n",
    "model.add(Dense(len(samples), activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "                optimizer='adam',\n",
    "                metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.77545293]\n[0.86206971]\n"
    }
   ],
   "source": [
    "# 11. Cimkék előállítása.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def conver_to_data(data, weights, samples, tokenizer):\n",
    "\n",
    "    lenv = len(data)\n",
    "    lens = len(samples[0])\n",
    "    lenss = len(samples)\n",
    "\n",
    "    weights_sim = cosine_similarity(weights)\n",
    "    weights_sim = 1 + weights_sim\n",
    "    weights_sim = weights_sim / 2\n",
    "    weights_sim = weights_sim / lens\n",
    "    similarities = []\n",
    "    for i in range(0, lenv):\n",
    "        calc_buffer = []\n",
    "        for n in range(0, lenss):\n",
    "            calc_buffer.append(0.0)\n",
    "\n",
    "        for j in range(0, lens):\n",
    "            for n in range(0, lenss):\n",
    "                calc_buffer[n] = calc_buffer[n] + weights_sim[data[i][j]][samples[n][j]] \n",
    "\n",
    "        similarities.append(calc_buffer)\n",
    "\n",
    "    return numpy.asarray(similarities)\n",
    "\n",
    "y_train_sim = conver_to_data(y_train,\n",
    "                            model.layers[0].get_weights()[0],\n",
    "                            tmp_samples,\n",
    "                            tokenizer)\n",
    "\n",
    "y_test_sim = conver_to_data(y_test,\n",
    "                            model.layers[0].get_weights()[0],\n",
    "                            tmp_samples,\n",
    "                            tokenizer)\n",
    "\n",
    "# A tanuló cimkék első elem\n",
    "print(y_train_sim[0])\n",
    "# A teszt cimkék első eleme\n",
    "print(y_test_sim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 56281 samples, validate on 14071 samples\nEpoch 1/5\n56200/56281 [============================>.] - ETA: 0s - loss: 1.0969e-04 - mae: 0.0079 - rmse: 0.0105\nEpoch 00001: val_loss improved from inf to 0.00011, saving model to model_001-0.007867-0.007672_pattern.h5\n56281/56281 [==============================] - 6s 107us/sample - loss: 1.0964e-04 - mae: 0.0079 - rmse: 0.0105 - val_loss: 1.0647e-04 - val_mae: 0.0077 - val_rmse: 0.0103\nEpoch 2/5\n56000/56281 [============================>.] - ETA: 0s - loss: 8.9053e-05 - mae: 0.0071 - rmse: 0.0094\nEpoch 00002: val_loss improved from 0.00011 to 0.00009, saving model to model_002-0.007104-0.006822_pattern.h5\n56281/56281 [==============================] - 6s 108us/sample - loss: 8.9024e-05 - mae: 0.0071 - rmse: 0.0094 - val_loss: 8.6836e-05 - val_mae: 0.0068 - val_rmse: 0.0093\nEpoch 3/5\n56000/56281 [============================>.] - ETA: 0s - loss: 7.5187e-05 - mae: 0.0065 - rmse: 0.0087\nEpoch 00003: val_loss improved from 0.00009 to 0.00008, saving model to model_003-0.006492-0.006367_pattern.h5\n56281/56281 [==============================] - 5s 95us/sample - loss: 7.5138e-05 - mae: 0.0065 - rmse: 0.0087 - val_loss: 7.5611e-05 - val_mae: 0.0064 - val_rmse: 0.0087\nEpoch 4/5\n55800/56281 [============================>.] - ETA: 0s - loss: 6.5742e-05 - mae: 0.0061 - rmse: 0.0081\nEpoch 00004: val_loss improved from 0.00008 to 0.00007, saving model to model_004-0.006071-0.005980_pattern.h5\n56281/56281 [==============================] - 6s 100us/sample - loss: 6.5687e-05 - mae: 0.0061 - rmse: 0.0081 - val_loss: 6.8242e-05 - val_mae: 0.0060 - val_rmse: 0.0083\nEpoch 5/5\n56000/56281 [============================>.] - ETA: 0s - loss: 5.8721e-05 - mae: 0.0057 - rmse: 0.0077\nEpoch 00005: val_loss improved from 0.00007 to 0.00006, saving model to model_005-0.005730-0.005684_pattern.h5\n56281/56281 [==============================] - 6s 104us/sample - loss: 5.8654e-05 - mae: 0.0057 - rmse: 0.0077 - val_loss: 6.2021e-05 - val_mae: 0.0057 - val_rmse: 0.0079\n{'loss': [0.0001096353160609087, 8.902429358011315e-05, 7.513758577925963e-05, 6.568711280961474e-05, 5.86536814472952e-05], 'mae': [0.007866511, 0.007103707, 0.0064922716, 0.0060714856, 0.005730026], 'rmse': [0.010470689, 0.00943527, 0.008668193, 0.008104758, 0.007658569], 'val_loss': [0.00010647106921880193, 8.683611422072082e-05, 7.561145189323825e-05, 6.824181160892164e-05, 6.202087426922888e-05], 'val_mae': [0.0076715313, 0.006822344, 0.0063669155, 0.005980037, 0.0056835855], 'val_rmse': [0.010318481, 0.00931859, 0.008695485, 0.008260859, 0.007875334]}\n"
    }
   ],
   "source": [
    "# 12.0 Model betanítása\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('model_{epoch:03d}-{mae:03f}-{val_mae:03f}_pattern.h5',\n",
    "                                 verbose=1,\n",
    "                                 monitor='val_loss',\n",
    "                                 save_best_only=True,\n",
    "                                 mode='auto')\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                y_train_sim,\n",
    "                epochs=5,\n",
    "                batch_size=100,\n",
    "                validation_data=(x_test, y_test_sim),\n",
    "                callbacks=[checkpoint],\n",
    "                verbose=1)\n",
    "\n",
    "# Tanulási történet kiíratása\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.98857635]\n [0.97920734]\n [0.92161614]\n [0.94661504]\n [0.9095697 ]\n [0.88668615]\n [0.8746072 ]\n [0.8607577 ]\n [0.84764856]\n [0.626014  ]\n [0.626014  ]\n [0.76784456]\n [0.642442  ]]\n"
    }
   ],
   "source": [
    "# 13.0 Model működésének tesztelése\n",
    "model = models.load_model(\"model_005-0.008607-0.008416_pattern.h5\", compile=False)\n",
    "\n",
    "test_samples = [\"az arteria carotis communisban % stenosist okozó echogen plaque.\",\n",
    "            \"az arteria carotis communisban % stenosist okozó meszes plaque.\",\n",
    "            \"arteria carotis communisban több % stenosist okozó meszes plaque.\",\n",
    "            \"a arteria carotis interna elején % stenosist okozó echogen plaque.\",\n",
    "            \"a arteria carotis interna elején % stenosist okozó meszes plaque.\",\n",
    "            \"carotis oszlásban (bifurcatioban) % stenosist okozó echogen plaque.\",\n",
    "            \"carotis oszlásban (bifurcatioban) % stenosist okozó meszes plaque.\",\n",
    "            \"a carotis externában % stenosist okozó echogen plaque.\",\n",
    "            \"a carotis externában % stenosist okozó meszes plaque.\",\n",
    "            \"arteria vertebralisban az áramlási sebesség a norm. érték alsó határának %-a.\",\n",
    "            \"arteria basilarisban az áramlási sebesség a norm. érték alsó határának %-a.\",\n",
    "            \"endarterectomia utáni állapot. az carotis communis egész hosszában 10-20\",\n",
    "            \"a. carotis externában áramlási sebesség alapján 50-60 %-os\"]\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "test_samples = tokenizer.texts_to_sequences(test_samples)\n",
    "test_samples = pad_sequences(test_samples,\n",
    "                            padding='post',\n",
    "                            maxlen=maxlen)\n",
    "\n",
    "# A model predikciója a mintával való egyezéssel\n",
    "# Látható, hogy az első minta egyezik meg a tanuló mintánkkal ezért ott a legerősebb a predikció. De vannak más minták is amik közel vannak hozzá. Feldolgozás során egy küszöbb értékkel tudjuk szabályozni, hogy mennyire szoros mintákat vizsgálunk.\n",
    "print(model.predict(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}